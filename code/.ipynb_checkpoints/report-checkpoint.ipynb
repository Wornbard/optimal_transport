{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9532bc33",
   "metadata": {},
   "source": [
    "# Theoretical background\n",
    "## Optimal transport\n",
    "Consider two histograms $a\\in \\Sigma_m\\equiv\\{ a\\in\\mathbb{R}^m_+ : \\sum_i a_i=1\\}$, and $b\\in \\Sigma_n\\equiv \\{ b\\in\\mathbb{R}^n_+ : \\sum_i b_i=1\\}$. In our application they will correspond to the distribution of brightness of respectively $n, m$ objects observed in two distinct images.\n",
    "\n",
    "Moreover suppose we have some cost associated with transporting a unit of mass between any two objects corresponding to the entries of the two histograms. This is e.g. the Euclidean distance between the centres of the two objects.\n",
    "Store these in a matrix $C\\in\\mathbb{R}_+^{m\\times n}$ with $C_{i,j}$ being the cost of transporting from $i$-th point in the first histogram, to $j$-th in the second one.\n",
    "\n",
    "Then we consider the problem of 'transporting' one histogram onto the other that minimises the transport cost. We can encode a transport plan in a matrix $P\\in\\mathbb{R}_+^{m\\times n}$ such that the sum of each row and column equals the appropriate index of one of the two histograms, i.e. $P\\in U(a,b)\\equiv\\{P\\in\\mathbb{R}_+^{m\\times n}P\\mathbb{1}_n=a, P^T\\mathbb{1}_m=b \\}$.\n",
    "\n",
    "Then the problem we are looking to solve is finding $L_C(a,b)=\\min\\langle C, P\\rangle \\equiv \\sum_{i,j}C_{i,j}P_{i,j}$ over $P\\in U(a,b)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6dc4af",
   "metadata": {},
   "source": [
    "## Unbalanced transport\n",
    "In many practical scenarios mass conservation may be violated. We may want to allow for that in our framework as follows. Given two histograms $a,b$, we will be considering transport between arbitrary histograms $\\tilde{a},\\tilde{b}$ in $\\Sigma_m,\\Sigma_n$ and additionally penalise the difference betwee those and the initial histograms using some divergences $D(a,\\tilde{a}) and D(b,\\tilde{b})$.\n",
    "\n",
    "Then we are looking to find\n",
    "$$\n",
    "L_C^\\tau (a,b)=\\min_{\\tilde{a},\\tilde{b}} L_c(\\tilde{a},\\tilde{b})+\\tau_1 D(a,\\tilde{a})+\\tau_2 D(b,\\tilde{b})= \\min_{P\\in\\mathbb{R}_+^{m\\times n}} \\langle C,P \\rangle+\\tau_1 D(P\\mathbb{1}_n , a)+\\tau_2 D(P^T\\mathbb{1}_m , b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24de960e",
   "metadata": {},
   "source": [
    "## Efficient computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1c661b",
   "metadata": {},
   "source": [
    "With an appropriate choice of a divergence, namely $D=KL$, the Kulback-Leibler divergence and an addition of a regularization term $\\epsilon H(P)$ to the minimised sum, we can solve the problem very efficiently. Here $H(P)\\equiv -\\sum_{i,j}P_{i,j}(log(P_{i,j})-1)$ is the discrete entropy of the coupling matrix.\n",
    "In this case one can show that the problem becomes convex, the optimal copling matrix is of the form $P_{i,j}=u_i,K_{i,j}v_j$ where $K_{i,j}=e^{-\\frac{C_{i,j}}{\\epsilon}}$ and the two vectors $u,v$ can be found with so called Sinkhorn iterations:\n",
    "\n",
    "$$\n",
    "u \\leftarrow \\left(\\frac{a}{Kv} \\right)^{\\frac{\\tau_1}{\\tau_1+\\epsilon}} \n",
    "$$\n",
    "$$\n",
    "v \\leftarrow \\left(\\frac{b}{K^T u} \\right)^{\\frac{\\tau_2}{\\tau_2+\\epsilon}} \n",
    "$$\n",
    " \n",
    "For more details consult e.g. [Gabriel Peyr√©, Marco Cuturi](https://arxiv.org/abs/1803.00567)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9fdd85",
   "metadata": {},
   "source": [
    "# Algorithm\n",
    "\n",
    "In a nutshell the way the program works as follows : \n",
    "\n",
    "Suppose we have a list of images of some system of point objects which we want to track.\n",
    "\n",
    "1. In an external program, locate the objects in every frame and return them in a dataframe that contains the frame number, position and brightness/some other mass-equivalent quantity of each object\n",
    "2. Accounting for the possibiity of an object being missed in some frames, say less than $\\Delta N$ of them at a time, compute a cost matrix C for every pair of frames whose number differs by no more then $\\Delta N$. The cost can be e.g. the square Euclidean distance between the objects (assuming the frame of reference is stationary)\n",
    "3. For any cost matrix C, solve the unbalanced OT problem and find the corresponding P\n",
    "4. Use these matrices to assign a weight to creating a 'connection' between observations in two frames i.e. identifying a pair of objects seen in them. Intuitively, the larger the entry $C_{i,j}$ of a given matrix, the more likely it is that we wish to connect the objects it corresponds to.\n",
    "5. Sort the possible connections by decreasing weight.\n",
    "6. While there are connections left to be considered, look at the lowest non-considered connection. If it is s.t. creating it would not interfere with any previously created tracks, then connect the two objects, otherwise reject it and move on.\n",
    "\n",
    "At the end of that procedue we are left with a dataframe that for each observation contains its track id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2453ef5",
   "metadata": {},
   "source": [
    "# Most important parameters\n",
    "\n",
    "1. $\\Delta N$ referred to in the code as *dt_max* is the maximum separation between frames we consider connecting\n",
    "2. $\\epsilon$ strength of the entropic regularization. For best results it should be as small as possible while preserving quick convergence. In a dataset where the entries of C_{i,j} are on the order of 100, setting it to rougly 0.1 seems to yield good results.\\\n",
    "TODO : How should this scale according to theory?\n",
    "3. $\\tau$ referred to in the code as *alpha* is the weight assigned to the divergence terms in unbalanced OT. We only consider the case of $\\tau_1=\\tau_2=\\tau$.\\\n",
    "Limiting cases : \\\n",
    " $\\tau\\rightarrow 0$ : modifying the measures is very cheap so the solution will be one where the measures just vanish, \\\n",
    " TODO : ensure I use the word 'measure' instead of 'distribution' everwhere\n",
    "4. Number of steps in Sinkhorn's algorithm. For a reasonable $\\epsilon$ we can get it down to ~500, usually not much lower.\\\n",
    "TODO : same as in point 2\n",
    "\n",
    "Some additional parameters may be set up in the program. Consult LINK TO TRACKER_NOTEBOOK here to to read about those"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7857bc",
   "metadata": {},
   "source": [
    "# Example application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1d942f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
